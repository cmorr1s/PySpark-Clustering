{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing customer flight data using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Cory Morris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SQLContext, SparkContext\n",
    "from pyspark.sql.types import DateType, FloatType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"The Notebook that I am using automatically intializes SparkContext.\n",
    "Therefore, usual you would need to run the following line of code to Create a SparkContext.\n",
    "\"\"\"\n",
    "# sc = SparkContext()\n",
    "\n",
    "## initialize SQL Contexts \n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = \"enter_your_path_here\"\n",
    "\n",
    "orig_data = (sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\")\\\n",
    "             .option(\"inferSchema\", \"true\").load(file_name))\n",
    "orig_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to get a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orig_data.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove rows with faulty Gendercode and BirthdateID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data = orig_data.where(col('birthdateid').isNotNull())\n",
    "orig_data = orig_data.where(col('GenderCode') != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace faulty values in Age column with median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data.fillna(40, subset=['Age'])\n",
    "orig_data = orig_data.withColumn(\"Age\", when(col('Age') < 0, 40).otherwise(col('Age')))\n",
    "orig_data = orig_data.withColumn(\"Age\", when(col('Age') > 120, 40).otherwise(col('Age')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace NAs in UflyRewardsNumber with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orig_data = orig_data.fillna(0, subset=['UFlyRewardsNumber'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace Missing values in UflyMemberStatus with \"Non-Fly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data = orig_data.withColumn('UflyMemberStatus', when(col(\"UflyMemberStatus\") == \"\", \"non-ufly\").\\\n",
    "                                   otherwise(col(\"UflyMemberStatus\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retaining only those rows which have single occurrence of PNRLocatorID, CouponSeqNbr, PaxName, ServiceStartCity, ServiceEndCity, ServiceStartDate combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = orig_data.groupBy(\"PNRLocatorID\", \"CouponSeqNbr\", \"PaxName\", \"ServiceStartCity\", \"ServiceEndCity\", \"ServiceStartDate\").\\\n",
    "                        agg(count(lit(1)).alias(\"num_records\"))\n",
    "orig_data = orig_data.join(df2, [\"PNRLocatorID\", \n",
    "                                   \"CouponSeqNbr\", \n",
    "                                   \"PaxName\", \n",
    "                                   \"ServiceStartCity\", \n",
    "                                   \"ServiceEndCity\", \n",
    "                                   \"ServiceStartDate\"]).filter(col(\"num_records\") ==1)\n",
    "orig_data.drop(col(\"test_id\")).drop(col(\"num_records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with faulty city codes as BookingChannel. Some rows have city names for Booking Channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data = orig_data.withColumn(\"BookingChannel\", when((col(\"BookingChannel\") != \"Outside Booking\") & \\\n",
    "                                                         (col(\"BookingChannel\") != \"SCA Website Booking\") &\\\n",
    "                                                         (col(\"BookingChannel\") != \"Tour Operator Portal\") &\\\n",
    "                                                         (col(\"BookingChannel\") != \"Reservations Booking\") &\\\n",
    "                                                         (col(\"BookingChannel\") != \"SY Vacation\"), \"Other\").\\\n",
    "                                   otherwise(col(\"BookingChannel\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with MarketingAirlineCode code other than \"SY\" *(the airline code for the airline of interest)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data = orig_data.where(col(\"MarketingAirlineCode\")=='SY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column called error which contains 1 if the PNR is errored or 0 otehrwise. (Error PNR refers to those which do not start with coupon sequence number 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = orig_data.groupBy(\"PNRLocatorID\").agg(when(min(col(\"CouponSeqNbr\")) != 1, 1).otherwise(0).alias(\"PNR_error\"))\n",
    "orig_data = orig_data.join(df3, [\"PNRLocatorID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retaining only the non-errored rows and check how many rows are remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data = orig_data.where(col(\"PNR_error\")==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation and Aggregation\n",
    "This section will aggregate records for each unique customer using the UID. This is needed since we want to cluster and therefore need one record of data for each customer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Age Group Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data = clean_data.withColumn(\"age_group\", when((col('Age')>=0) & (col('Age')<18), \"0-17\").\\\n",
    "                                   when((col('Age')>=18) & (col('Age')<25), \"18-24\").\\\n",
    "                                  when((col('Age')>=25) & (col('Age')<35), \"25-34\").\\\n",
    "                                  when((col('Age')>=35) & (col('Age')<55), \"35-54\").\\\n",
    "                                  when((col('Age')>=55), \"55+\").otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the True Origins for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_origins = clean_data.sort(col('PNRLocatorID').asc()).sort(col('PaxName').asc()).sort(col('CouponSeqNbr').asc())\n",
    "true_origins = true_origins.groupBy('PNRLocatorID', 'PaxName').agg(first(\"ServiceStartCity\").alias(\"true_origin\")).distinct()\n",
    "\n",
    "aggregate_data = clean_data.join(true_origins, [\"PNRLocatorID\", \"PaxName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the True Destinations for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_destination = clean_data.sort(col('PNRLocatorID').asc()).sort(col('PaxName').asc()).sort(col('CouponSeqNbr').asc())\n",
    "final_destination = final_destination.groupBy('PNRLocatorID', 'PaxName').agg(last(\"ServiceEndCity\")\\\n",
    "                                                                             .alias(\"final_destination\")).distinct()\n",
    "\n",
    "aggregate_data = aggregate_data.join(final_destination, [\"PNRLocatorID\",\"PaxName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Service Start date to Date type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregate_data = aggregate_data.withColumn(\"ServiceStartDate\", (col(\"ServiceStartDate\").cast('date')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the place of maximum stay during the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(aggregate_data.PNRLocatorID, aggregate_data.PaxName)\\\n",
    "                    .orderBy(aggregate_data.ServiceStartDate)\n",
    "aggregate_data = aggregate_data.withColumn(\"lead_Date\", lead(aggregate_data.ServiceStartDate, 1).over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregate_data = aggregate_data.withColumn(\"stay\", datediff(aggregate_data.lead_Date, aggregate_data.ServiceStartDate))\n",
    "aggregate_data = aggregate_data.withColumn(\"stay\", when(isnull(aggregate_data.stay), 0).otherwise(aggregate_data.stay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stay = aggregate_data.groupBy(\"PNRLocatorID\", \"PaxName\").agg(max(col(\"stay\")).alias(\"max_stay\"))\n",
    "\n",
    "# merge back to aggregate_data dataframe\n",
    "aggregate_data = aggregate_data.join(df_stay, [\"PNRLocatorID\", \"PaxName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = aggregate_data.groupBy(\"PNRLocatorID\", \"PaxName\").agg(first(when(col(\"stay\")==col(\"max_stay\"),\\\n",
    "                                                                col(\"ServiceEndCity\"))).alias(\"true_destination\"))\n",
    "aggregate_data = aggregate_data.join(df, [\"PNRLocatorID\", \"PaxName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, determine if the trip was a one-way or round-trip. The trip is considered a round trip if the service end city (Final Destination) will be the same as the service start city (True Origin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregate_data = aggregate_data.withColumn(\"round_trip\", when(col(\"true_origin\")==col(\"final_destination\"), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, determine the group size, the number of people who traveled together in each trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grp_size = aggregate_data.groupBy(\"PNRLocatorID\").agg(countDistinct(col(\"uid\")).alias(\"group_size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregate_data = aggregate_data.join(grp_size, \"PNRLocatorID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a binary inidcator for whether the group-size was 1 person or more, i.e. flight was flown by a single customer.\n",
    "*(0 = group size of 1 customer,\n",
    "1 = group size was more than 1 customer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregate_data = aggregate_data.withColumn(\"group\", when(col(\"group_size\")>1,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, handle seasonality in terms of quaters. Assign Q1 to Q4 based on the quarter of the year in which the trip was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregate_data = aggregate_data.withColumn(\"seasonality\", \n",
    "                                    when((month(col('ServiceStartDate'))>=1) & (month(col('ServiceStartDate'))<=3), \"Q1\").\\\n",
    "                                   when((month(col('ServiceStartDate'))>=4) & (month(col('ServiceStartDate'))<=6), \"Q2\").\\\n",
    "                                  when((month(col('ServiceStartDate'))>=7) & (month(col('ServiceStartDate'))<=9), \"Q3\").\\\n",
    "                                  when((month(col('ServiceStartDate'))>=10) & (month(col('ServiceStartDate'))<=12), \"Q4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, calculate the number of days the ticket was booked in advance. It is the difference between PNRCreateDate and ServiceStartDate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregate_data = aggregate_data.withColumn(\"PNRCreateDate\", (col(\"PNRCreateDate\").cast('date')))\n",
    "\n",
    "aggregate_data = aggregate_data.withColumn(\"days_pre_booked\", floor(datediff(aggregate_data.ServiceStartDate, \n",
    "                                                                             aggregate_data.PNRCreateDate)))\n",
    "\n",
    "aggregate_data = aggregate_data.withColumn(\"PostalCode\", col(\"PostalCode\").cast('int'))\n",
    "aggregate_data = aggregate_data.withColumn(\"EnrollDate\", col(\"EnrollDate\").cast('date'))\n",
    "aggregate_data = aggregate_data.withColumn(\"MarketingFlightNbr\", col(\"MarketingFlightNbr\").cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by PNR, UID and PaxName to get Final Dataset to use in Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_agg_df = aggregate_data.groupBy('PNRLocatorID','uid','PaxName').agg(first('ServiceStartDate').alias(\"ServiceStartDate\"),\\\n",
    "                                                                first(\"BookingChannel\").alias(\"BookingChannel\"),\\\n",
    "                                                                mean(\"TotalDocAmt\").alias(\"Avg_Amount\"),\\\n",
    "                                                                first(\"UFlyRewardsNumber\").alias(\"UFlyRewardsNum\"),\\\n",
    "                                                                first(\"UflyMemberStatus\").alias(\"UFly_Status\"),\\\n",
    "                                                                last(\"age_group\").alias(\"Age_Group\"),\\\n",
    "                                                                first(\"true_origin\").alias(\"True_Origin\"),\\\n",
    "                                                                first(\"true_destination\").alias(\"True_Destination\"),\\\n",
    "                                                                first(\"round_trip\").alias(\"Round_Trip\"),\\\n",
    "                                                                first(\"group_size\").alias(\"Group_Size\"),\\\n",
    "                                                                first(\"group\").alias(\"Group\"),\\\n",
    "                                                                last(\"seasonality\").alias(\"Seasonality\"),\\\n",
    "                                                                max(\"days_pre_booked\").alias(\"Days_pre_Booked\"))\n",
    "\n",
    "final_agg_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Aggregated Data into RDD\n",
    "\n",
    "This is needed as Spark RDD's (Spark MLlib) have more options when it comes to clustering algorithms than do dataframes (Spark ML). \n",
    "\n",
    "*Other clustering algorithms were used but are not shown here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sunRDD = final_agg_df.rdd.map(lambda row: (row[0:16]))\n",
    "sunRDD_req = final_agg_df.rdd.map(lambda row: (row[5], row[12], row[15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Data using Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_amount_spent = sunRDD_req.map(lambda x: x[0])\n",
    "group_size = sunRDD_req.map(lambda x: x[1])\n",
    "booking_days = sunRDD_req.map(lambda x: x[2])\n",
    "\n",
    "avg_amount_min = avg_amount_spent.min()\n",
    "avg_amount_max = avg_amount_spent.max()\n",
    "group_size_min = group_size.min()\n",
    "group_size_max = group_size.max()\n",
    "booking_days_min = booking_days.min()\n",
    "booking_days_max = booking_days.max()\n",
    "\n",
    "sunRDD_norm = sunRDD_req.map(lambda row: ((float(row[0])-avg_amount_min)/(avg_amount_max - avg_amount_min),\n",
    "                                          (float(row[1])-group_size_min)/(group_size_max-group_size_min),\n",
    "                                          (float(row[2])-booking_days_min)/(booking_days_max-booking_days_min)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "clusters = KMeans.train(sunRDD_norm, 4, maxIterations=100, initializationMode='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the cluster label to each RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_id = clusters.predict(sunRDD_norm)\n",
    "sunRDD_final = sunRDD.zip(cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sunRDD_final = sunRDD_final.map(lambda line: (line[0][0],\n",
    "                                             line[0][1],\n",
    "                                             line[0][2],\n",
    "                                             line[0][3],\n",
    "                                             line[0][4],\n",
    "                                             line[0][5],\n",
    "                                             line[0][6],\n",
    "                                             line[0][7],\n",
    "                                             line[0][8],\n",
    "                                             line[0][9],\n",
    "                                             line[0][10],\n",
    "                                             line[0][11],\n",
    "                                             line[0][12],\n",
    "                                             line[0][13],\n",
    "                                             line[0][14],\n",
    "                                             line[0][15],\n",
    "                                             line[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert RDD's back to Dataframe\n",
    "\n",
    "I converted the RDD's back to a Dataframe for the team to then visualize the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sun_final_df = sunRDD_final.map(lambda p: Row(PNRLocatorID = (p[0]),\n",
    "                                             uid = (p[1]),\n",
    "                                             PaxName = (p[2]),\n",
    "                                             ServiceStartDate = (p[3]),\n",
    "                                             BookingChannel = (p[4]),\n",
    "                                             Avg_Amount_Spent = float(p[5]),\n",
    "                                             UFlyRewardsNum = (p[6]),\n",
    "                                             UFly_Status = (p[7]),\n",
    "                                             Age_Group = (p[8]),\n",
    "                                             True_Origin = (p[9]),\n",
    "                                             True_Destination = (p[10]),\n",
    "                                             Round_Trip = (p[11]),\n",
    "                                             Group_Size = int(p[12]),\n",
    "                                             Group_Flag = (p[13]),\n",
    "                                             Season = (p[14]),\n",
    "                                             Days_pre_Booked = int(p[15]),\n",
    "                                             Cluster = (p[16])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sun_final_df.limit(5).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
